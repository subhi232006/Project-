# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ejpeMJhYQ9nF2Hsj3PkPeoTU-_0ofHCQ
"""

import pandas as pd # Make sure pandas is imported

df = pd.read_csv('city_day.csv.zip')

df.head()

print("Shape:", df.shape)

print("Columns:", df.columns.tolist())

df.info()

df.describe()

print(df.isnull().sum())

import zipfile
import pandas as pd

# Step 1: Unzip the uploaded file
zip_path = '/content/city_day.csv.zip'
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('/content/')

# Step 2: Load the CSV file (update filename if different)
csv_path = '/content/city_day.csv'
df = pd.read_csv(csv_path)

# Step 3: Show basic info
print("Dataset Shape:", df.shape)
print("\nFirst 5 rows:\n", df.head())

# Step 4: Check for missing values
print("\nMissing Values (column-wise):")
print(df.isnull().sum())

# Step 5: Check for duplicate rows
duplicate_rows = df[df.duplicated()]
print(f"\nNumber of duplicate rows: {duplicate_rows.shape[0]}")

# Optional: Show duplicate rows if any
if not duplicate_rows.empty:
    print("\nDuplicate rows:")
    print(duplicate_rows)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset again (if not already loaded)
df = pd.read_csv('/content/city_day.csv')

# Show column names to pick features
print("Columns in dataset:\n", df.columns)

# Convert 'Date' column to datetime if it's present
if 'Date' in df.columns:
    df['Date'] = pd.to_datetime(df['Date'])

# --- 1. Histogram for a numeric feature (e.g., PM2.5) ---
plt.figure(figsize=(8, 5))
sns.histplot(df['PM2.5'].dropna(), bins=30, kde=True)
plt.title('Distribution of PM2.5')
plt.xlabel('PM2.5')
plt.ylabel('Frequency')
plt.show()

# --- 2. Boxplot of a feature grouped by city ---
plt.figure(figsize=(12, 6))
top_cities = df['City'].value_counts().index[:5]
sns.boxplot(x='City', y='PM2.5', data=df[df['City'].isin(top_cities)])
plt.title('PM2.5 Distribution in Top 5 Cities')
plt.show()

# --- 3. Line plot over time (e.g., PM10 for a specific city) ---
plt.figure(figsize=(10, 5))
sample_city = 'Delhi'
city_data = df[df['City'] == sample_city]
sns.lineplot(x='Date', y='PM10', data=city_data)
plt.title(f'PM10 Levels Over Time in {sample_city}')
plt.xlabel('Date')
plt.ylabel('PM10')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# --- 4. Correlation heatmap for numeric features ---
plt.figure(figsize=(10, 8))
numeric_df = df.select_dtypes(include='number')
sns.heatmap(numeric_df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap of Numeric Features')
plt.show()

import pandas as pd

# Load dataset
df = pd.read_csv('/content/city_day.csv')

# Show column names
print("Columns in dataset:")
print(df.columns.tolist())

# Step 1: Define target column
# You can change 'AQI' to any other column you want to predict
target_column = 'AQI'

# Step 2: Drop target and irrelevant columns to get feature set
# Optional: Exclude columns like 'Date' and 'City' if not usable directly
irrelevant_columns = ['Date', 'City']  # Optional, based on use case

# Final feature set
features = df.drop(columns=[target_column] + irrelevant_columns, errors='ignore')

# Step 3: Extract target values
target = df[target_column]

# Display the results
print(f"\nTarget column: {target_column}")
print(f"\nFeature columns:\n{features.columns.tolist()}")
print("\nSample Features:")
print(features.head())

print("\nSample Target:")
print(target.head())

categorical_cols = df.select_dtypes(include=['object']).columns
print("Categorical Columns:", categorical_cols.tolist())

df_encoded = pd.get_dummies(df, drop_first=True)

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load dataset
df = pd.read_csv('/content/city_day.csv')

# Choose target and drop unnecessary columns
target_column = 'AQI'
irrelevant_columns = ['Date', 'City']
features = df.drop(columns=[target_column] + irrelevant_columns, errors='ignore')

# Drop rows with missing values for simplicity
features_clean = features.dropna()

# --- Before scaling, convert non-numeric columns to numeric ---
# Select only numeric features for scaling
numeric_features = features_clean.select_dtypes(include=['number'])

# --- Option 1: StandardScaler (mean = 0, std = 1) ---
standard_scaler = StandardScaler()
features_standard_scaled = standard_scaler.fit_transform(numeric_features)  # Scale only numeric features


# --- Option 2: MinMaxScaler (scales between 0 and 1) ---
minmax_scaler = MinMaxScaler()
features_minmax_scaled = minmax_scaler.fit_transform(numeric_features)  # Scale only numeric features

# Convert scaled arrays back to DataFrames
features_standard_scaled_df = pd.DataFrame(features_standard_scaled, columns=numeric_features.columns)
features_minmax_scaled_df = pd.DataFrame(features_minmax_scaled, columns=numeric_features.columns)

# Display the results
print("Standard Scaled Features (first 5 rows):")
print(features_standard_scaled_df.head())

print("\nMin-Max Scaled Features (first 5 rows):")
print(features_minmax_scaled_df.head())

import pandas as pd
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv('/content/city_day.csv')

# Set target and feature columns
target_column = 'AQI'
irrelevant_columns = ['Date', 'City']
features = df.drop(columns=[target_column] + irrelevant_columns, errors='ignore')
target = df[target_column]

# Drop rows with missing values in both features and target
data = pd.concat([features, target], axis=1).dropna()
features_clean = data.drop(columns=[target_column])
target_clean = data[target_column]

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(
    features_clean, target_clean, test_size=0.2, random_state=42
)

# Display shapes of the split data
print("Training features shape:", X_train.shape)
print("Testing features shape:", X_test.shape)
print("Training target shape:", y_train.shape)
print("Testing target shape:", y_test.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load dataset
df = pd.read_csv('/content/city_day.csv')

# Step 2: Define target and drop irrelevant columns
target_column = 'AQI'
irrelevant_columns = ['Date', 'City']  # Drop 'City' because it's categorical

# Step 3: Drop rows with missing values (in any column)
df = df.dropna()

# Step 4: Define features and target
X = df.drop(columns=[target_column] + irrelevant_columns, errors='ignore')
y = df[target_column]

# Step 5: Ensure only numeric columns
X = X.select_dtypes(include='number')

# Step 6: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step 7: Model training
model = LinearRegression()
model.fit(X_train, y_train)

# Step 8: Prediction and evaluation
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("✅ Model Trained Successfully!")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R² Score: {r2:.2f}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load and clean dataset
df = pd.read_csv('/content/city_day.csv')
df = df.dropna()
target_column = 'AQI'
irrelevant_columns = ['Date', 'City']
X = df.drop(columns=[target_column] + irrelevant_columns, errors='ignore').select_dtypes(include='number')
y = df[target_column]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# --- Evaluation Metrics ---
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("✅ Evaluation Results:")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

# --- Visualization: Actual vs Predicted ---
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # perfect line
plt.xlabel('Actual AQI')
plt.ylabel('Predicted AQI')
plt.title('Actual vs Predicted AQI')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# Sample: Features used in the trained model
# You must use the same features and order as used in training
feature_names = X.columns.tolist()  # assuming X from earlier split is available
print("Required feature order:\n", feature_names)

# Example: New input (replace values with real data)
# Make sure the order and number of inputs match `feature_names`
new_data = pd.DataFrame([[
    50,   # PM2.5
    80,   # PM10
    10,   # NO
    20,   # NO2
    5,    # NOx
    2,    # NH3
    0.5,  # CO
    25,   # SO2
    30,   # O3
    0.02, # Benzene
    0.001,# Toluene
    0.005 # Xylene
]], columns=feature_names)

# Predict using trained model
predicted_aqi = model.predict(new_data)

print(f"\n✅ Predicted AQI for the new input: {predicted_aqi[0]:.2f}")

# Import necessary libraries
import pandas as pd
import zipfile
import os

# Unzip the file
zip_path = '/content/city_day.csv.zip'  # Adjust this if path changes
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('/content/')

# Load the dataset
csv_path = '/content/city_day.csv'
df = pd.read_csv(csv_path)

# Display the first few rows
print("Initial Data:")
print(df.head())

# Check data types and missing values
print("\nData Info:")
print(df.info())

# Fill or drop missing values (simple strategy: drop for now)
df.dropna(inplace=True)

# Encode categorical columns using Label Encoding or One-Hot Encoding
# Let's check which columns are categorical
cat_cols = df.select_dtypes(include=['object']).columns
print("\nCategorical Columns:")
print(cat_cols)

# Apply one-hot encoding
df_encoded = pd.get_dummies(df, columns=cat_cols)

# Show final shape and preview
print("\nEncoded Data Shape:", df_encoded.shape)
print(df_encoded.head())

# Save the encoded dataset if needed
df_encoded.to_csv('/content/encoded_city_day.csv', index=False)

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the encoded dataset
# Corrected file path:
df = pd.read_csv('/content/city_day.csv')

# Set your target column — change 'AQI' to your

!pip install gradio
import gradio as gr
import pickle
import numpy as np
from sklearn.ensemble import RandomForestRegressor # Import RandomForestRegressor
from sklearn.model_selection import train_test_split #Import train_test_split
import pandas as pd  #Import pandas



# Assuming 'df' from previous cell
target_column = 'AQI'
irrelevant_columns = ['Date', 'City']  # Drop 'City' because it's categorical
# Drop rows with missing values (in any column)
df = df.dropna()
# Define features and target
X = df.drop(columns=[target_column] + irrelevant_columns, errors='ignore')
y = df[target_column]
# Ensure only numeric columns
X = X.select_dtypes(include='number')
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# Initialize and train the model
model = RandomForestRegressor(n_estimators=100, random_state=42) # Initialize the RandomForestRegressor here
model.fit(X_train, y_train) # Train the Model
feature_names = X.columns.tolist() # Get feature names from X


# Save the model
with open("model.pkl", "wb") as model_file:
    pickle.dump(model, model_file)

# Save feature names
with open("features.pkl", "wb") as feat_file:
    pickle.dump(feature_names, feat_file)


# Define prediction function
def predict_aqi(*args):
    input_array = np.array(args).reshape(1, -1)
    prediction = model.predict(input_array)[0]
    return f"Predicted AQI: {round(prediction, 2)}"

# Create input components
inputs = [gr.Number(label=feature) for feature in feature_names]

# Build the Gradio interface
interface = gr.Interface(
    fn=predict_aqi,
    inputs=inputs,
    outputs="text",
    title="AQI Prediction App",
    description="Enter values for each feature to predict the Air Quality Index."
)

# Launch app
interface.launch(share=True)

#Now you can load the saved model:
with open("model.pkl", "rb") as f:
    model = pickle.load(f)